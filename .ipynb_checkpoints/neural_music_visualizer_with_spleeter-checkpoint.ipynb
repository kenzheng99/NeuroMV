{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3015,
     "status": "ok",
     "timestamp": 1651427513821,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "Dx1ZaG7Avx3M",
    "outputId": "9a850cde-a4ac-4d42-bb13-83c1c78b8088"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb, rgb_to_hsv\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL85Obdw8hEu"
   },
   "source": [
    "# CPPN implementation\n",
    "\n",
    "Notes:\n",
    "- increasing the number of layers increases visual complexity and sharpness\n",
    "- increasing the hidden size also increases complexity/sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8839,
     "status": "ok",
     "timestamp": 1651427556849,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "pc4zGXfx8aro",
    "outputId": "c8f0dae1-e891-4235-c1f9-bef70f77c296"
   },
   "outputs": [],
   "source": [
    "# CPPN network\n",
    "\n",
    "def CPPN(hidden_size=32, latent_size=20):\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(3 + latent_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(hidden_size, hidden_size),\n",
    "        nn.Tanh(),\n",
    "        # nn.Linear(hidden_size, hidden_size),\n",
    "        # nn.Tanh(),\n",
    "        # nn.Linear(hidden_size, hidden_size),\n",
    "        # nn.Tanh(),\n",
    "        # nn.Linear(hidden_size, hidden_size),\n",
    "        # nn.Tanh(),\n",
    "        nn.Linear(hidden_size, 3),\n",
    "        nn.Sigmoid(),\n",
    "    ).to(device)\n",
    "\n",
    "    # re-initialize linear layers with standard Gaussian distribution\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(model), 2):\n",
    "            model[i].weight = torch.nn.Parameter(torch.randn_like(model[i].weight))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = CPPN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651427556849,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "D3FTH-DT81B3"
   },
   "outputs": [],
   "source": [
    "# helper function to generate inputs to CPPN\n",
    "\n",
    "def gen_cppn_input(batch_size=1, x_dim=32, y_dim=32, scale=1.0, latents=None, latent_scale=1.0):\n",
    "    '''\n",
    "    calculates and returns a vector of x and y coordintes, and corresponding radius from the centre of image.\n",
    "    latents is a vector of size (batch_size x latent_size)\n",
    "    TODO finish documentation\n",
    "    '''\n",
    "    n_points = x_dim * y_dim\n",
    "    x_range = scale*(np.arange(x_dim)-(x_dim-1)/2.0)/(x_dim-1)/0.5\n",
    "    y_range = scale*(np.arange(y_dim)-(y_dim-1)/2.0)/(y_dim-1)/0.5\n",
    "    x_mat = np.matmul(np.ones((y_dim, 1)), x_range.reshape((1, x_dim)))\n",
    "    y_mat = np.matmul(y_range.reshape((y_dim, 1)), np.ones((1, x_dim)))\n",
    "    r_mat = np.sqrt(x_mat*x_mat + y_mat*y_mat)\n",
    "    x_mat = np.tile(x_mat.flatten(), batch_size).reshape(batch_size, n_points, 1)\n",
    "    y_mat = np.tile(y_mat.flatten(), batch_size).reshape(batch_size, n_points, 1)\n",
    "    r_mat = np.tile(r_mat.flatten(), batch_size).reshape(batch_size, n_points, 1)\n",
    "    r_mat = np.zeros_like(r_mat)\n",
    "    print(np.max(r_mat))\n",
    "\n",
    "    assert len(latents) == batch_size\n",
    "    latent_size = latents.shape[1]\n",
    "    latents *= latent_scale\n",
    "    latents_tiled = np.tile(np.reshape(latents, [batch_size, 1, latent_size]), [1, n_points, 1])\n",
    "\n",
    "    input = np.concatenate([x_mat, y_mat, r_mat, latents_tiled], axis=2)\n",
    "    return torch.tensor(input, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "error",
     "timestamp": 1651427710103,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "Riv30SyETscc",
    "outputId": "4f08901e-644c-4c6f-9074-e09d80a004aa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_training_image(img_path, latent_size, coord_scale=1, resolution=200, use_hsv=False):\n",
    "    raw_img = Image.open(img_path).convert('RGB')\n",
    "    raw_width, raw_height = raw_img.size\n",
    "    aspect = raw_width / raw_height\n",
    "    raw_img = raw_img.resize((int(resolution * aspect), resolution));\n",
    "    img = np.array(raw_img) / 255.\n",
    "    if use_hsv:\n",
    "        img = rgb_to_hsv(img)\n",
    "    width, height = img.shape[1], img.shape[0] \n",
    "    input = gen_cppn_input(batch_size=1, x_dim=width, y_dim=height, scale=coord_scale, latents=np.zeros((1, latent_size)), latent_scale=1)[0]\n",
    "    labels = torch.tensor(img, dtype=torch.float).reshape((width * height, 3))\n",
    "\n",
    "    return input, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SGF46v28jvG"
   },
   "source": [
    "# Audio I/O and source separation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44826,
     "status": "ok",
     "timestamp": 1651427787712,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "Qb8-BVJIKY9b",
    "outputId": "ee825001-4347-4c78-e9d6-a15ff744f5bf"
   },
   "outputs": [],
   "source": [
    "# download audio file from youtube\n",
    "# to use your own audio file, upload it as /content/audio.wav\n",
    "url = \"https://www.youtube.com/watch?v=BtvJaNeELic\"\n",
    "\n",
    "!pip install youtube-dl\n",
    "!youtube-dl -f \"bestaudio[ext=m4a]\" {url} --output \"audio.m4a\"\n",
    "!ffmpeg -i audio.m4a audio.wav -loglevel 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into vocals+accompaniment\n",
    "# IMPORTANT: only works on CPU (if using GPU on the notebook, perform source separation on a separate CPU instance then upload the separated audio files)\n",
    "if device == \"cpu\":\n",
    "    !pip install spleeter\n",
    "    !spleeter separate audio.wav\n",
    "else:\n",
    "    print(\"Please use a CPU machine to perform source separation\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "executionInfo": {
     "elapsed": 1072,
     "status": "ok",
     "timestamp": 1651427898458,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "YnN4OoiDxKW_",
    "outputId": "99d2d4e6-65a2-4105-d572-9e5fe612927f"
   },
   "outputs": [],
   "source": [
    "# audio parameters\n",
    "audio_path = 'vocals.wav' # REPLACE with 'accompaniment.wav' for the accompaniment\n",
    "target_fps = 30 # number of frames per second for output video\n",
    "latent_size = 16 # frequency resolution (number of mel bins) for audio frame vectors\n",
    "\n",
    "# load audio file\n",
    "audio, sr = torchaudio.load(audio_path, normalize=True)\n",
    "audio = audio[0]\n",
    "print('sample rate', sr)\n",
    "print('audio shape', audio.shape)\n",
    "\n",
    "# feature extraction with a Mel spectrogram\n",
    "hop_length = int(sr / target_fps)\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=hop_length*2, n_mels=latent_size, hop_length=hop_length, win_length=hop_length * 2)\n",
    "spectrogram = transform(audio)\n",
    "spectrogram = spectrogram.T\n",
    "spectrogram = spectrogram / torch.max(spectrogram) # normalize to range [0,1]\n",
    "print('spectrogram shape', spectrogram.shape)\n",
    "\n",
    "# visualize spectrogram\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.imshow(spectrogram.T[:, 0:200])\n",
    "\n",
    "# play audio\n",
    "# Audio(data=audio, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-00GwYXK9D56"
   },
   "source": [
    "# Generate Output Video\n",
    "\n",
    "run this cell multiple times to get a good looking base image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1651427902365,
     "user": {
      "displayName": "Kayo Yin",
      "userId": "10146778865848170370"
     },
     "user_tz": 240
    },
    "id": "2Dj_gjtwG_FS",
    "outputId": "ba38951b-6536-4adc-85f1-d7c15b817068"
   },
   "outputs": [],
   "source": [
    "# output video parameters\n",
    "x_dim = 400\n",
    "y_dim = 400\n",
    "\n",
    "# cppn parameters\n",
    "coord_scale = 0.2\n",
    "latent_scale = 0.5\n",
    "use_hsv = False # whether to used the HSV color space, usually True for abstract art and False for using training images\n",
    "\n",
    "# initialize CPPN\n",
    "model = CPPN(hidden_size=32, latent_size=latent_size)\n",
    "\n",
    "# generate test image with zero latent vector\n",
    "input = gen_cppn_input(batch_size=1, x_dim=x_dim, y_dim=y_dim, scale=coord_scale, latents=np.zeros([1, latent_size]), latent_scale = latent_scale)\n",
    "output = model(input)\n",
    "output_img = torch.reshape(output, [y_dim, x_dim, 3]).cpu().detach().numpy()\n",
    "if use_hsv:\n",
    "    output_img = hsv_to_rgb(output_img)\n",
    "plt.imshow(output_img)\n",
    "print('base image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 11463,
     "status": "ok",
     "timestamp": 1651358553551,
     "user": {
      "displayName": "Kenneth Zheng",
      "userId": "05072488123980564723"
     },
     "user_tz": 240
    },
    "id": "ehpVWZPQUqFA",
    "outputId": "1f1ded34-daf6-45fb-f83f-f484cd09d38a"
   },
   "outputs": [],
   "source": [
    "# train CPPN to match a reference image\n",
    "from IPython.display import clear_output\n",
    "\n",
    "image = 'vocals.png' # REPLACE with 'accompaniment.png' for the accompaniment image\n",
    "\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4);\n",
    "\n",
    "inputs, labels = load_training_image(image, latent_size, coord_scale=coord_scale, use_hsv=use_hsv)\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "n_epochs = 1000 # don't set this too high, otherwise model will overfit and latent vector will not change anything\n",
    "prev_loss = 0\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    output = model(inputs)\n",
    "    loss = criterion(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0: # display loss and show image every 100 epochs\n",
    "        print(f'epoch {epoch}: loss={loss}, diff={loss - prev_loss}')\n",
    "        prev_loss = loss\n",
    "        plt.figure(1, figsize=(10, 5))\n",
    "        output_img = torch.reshape(output, [200, 200, 3]).cpu().detach().numpy()\n",
    "        if use_hsv:\n",
    "            output_img = hsv_to_rgb(output_img)\n",
    "        plt.imshow(output_img)\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "print('Finished Training')\n",
    "\n",
    "input = gen_cppn_input(batch_size=1, x_dim=x_dim, y_dim=y_dim, scale=coord_scale, latents=np.zeros([1, latent_size]), latent_scale = latent_scale)\n",
    "output = model(input)\n",
    "output_img = torch.reshape(output, [y_dim, x_dim, 3]).cpu().detach().numpy()\n",
    "if use_hsv:\n",
    "    output_img = hsv_to_rgb(output_img)\n",
    "plt.imshow(output_img)\n",
    "print('After Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1651358575318,
     "user": {
      "displayName": "Kenneth Zheng",
      "userId": "05072488123980564723"
     },
     "user_tz": 240
    },
    "id": "tF-vnphhCykV",
    "outputId": "d2fe9b46-3561-433a-94da-c561065c94a8"
   },
   "outputs": [],
   "source": [
    "# generate some test frames\n",
    "num_frames = 8\n",
    "latent_scale = 0.5\n",
    "\n",
    "# test sequential frames to check smoothness\n",
    "print('sequential frames (above)')\n",
    "\n",
    "offset = 100\n",
    "frames = spectrogram[offset:offset + num_frames, :]\n",
    "\n",
    "input = gen_cppn_input(batch_size=num_frames, x_dim=x_dim, y_dim=y_dim, scale=coord_scale, latents=frames, latent_scale = latent_scale)\n",
    "output = model(input)\n",
    "output_imgs = torch.reshape(output, [num_frames, y_dim, x_dim, 3]).cpu().detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, num_frames, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    if use_hsv:\n",
    "        ax.imshow(hsv_to_rgb(output_imgs[i]))\n",
    "    else:\n",
    "        ax.imshow(output_imgs[i])\n",
    "\n",
    "# test random frames to check variation\n",
    "print('random frames (below)')\n",
    "frame_indices = np.random.choice(spectrogram.shape[0], num_frames, replace=False)\n",
    "frames = spectrogram[frame_indices, :]\n",
    "input = gen_cppn_input(batch_size=num_frames, x_dim=x_dim, y_dim=y_dim, scale=coord_scale, latents=frames, latent_scale = latent_scale)\n",
    "output = model(input)\n",
    "output_imgs = torch.reshape(output, [num_frames, y_dim, x_dim, 3]).cpu().detach().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, num_frames, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes):\n",
    "    if use_hsv:\n",
    "        ax.imshow(hsv_to_rgb(output_imgs[i]))\n",
    "    else:\n",
    "        ax.imshow(output_imgs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 815230,
     "status": "ok",
     "timestamp": 1651359433595,
     "user": {
      "displayName": "Kenneth Zheng",
      "userId": "05072488123980564723"
     },
     "user_tz": 240
    },
    "id": "ttMin58P-SC8",
    "outputId": "c5bf1ed8-e76a-45c5-805c-da1b1d76406a"
   },
   "outputs": [],
   "source": [
    "# generate all frames\n",
    "output_dir = '/content/img/'\n",
    "%rm -r /content/img/\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print('generating frames')\n",
    "for i, embedding in tqdm(enumerate(spectrogram), total=len(spectrogram)):\n",
    "    latents = embedding[np.newaxis, :]\n",
    "    input = gen_cppn_input(batch_size=1, x_dim=x_dim, y_dim=y_dim, scale=coord_scale, latents=latents, latent_scale=latent_scale)\n",
    "    output = model(input)\n",
    "    img = output[0].cpu().detach().numpy().reshape([y_dim, x_dim, 3])\n",
    "    if use_hsv:\n",
    "        img = hsv_to_rgb(img)\n",
    "    plt.imsave(f'/content/img/{i:04d}.png', img)\n",
    "\n",
    "# this cell can be stopped after a desired number of frames have been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79697,
     "status": "ok",
     "timestamp": 1651361546505,
     "user": {
      "displayName": "Kenneth Zheng",
      "userId": "05072488123980564723"
     },
     "user_tz": 240
    },
    "id": "dZ9mDuPiM9Rs",
    "outputId": "6de89ab1-c755-4b28-b338-030d28ee011a"
   },
   "outputs": [],
   "source": [
    "# generate video from image sequence\n",
    "# values in {brackets} are python variables\n",
    "print('generating output video')\n",
    "!ffmpeg -r {target_fps} -f image2 -s {x_dim}x{y_dim} -i /content/img/%04d.png -vcodec libx264 -crf 25 -pix_fmt yuv420p -loglevel 0 -nostats temp.mp4\n",
    "\n",
    "# add audio track\n",
    "print('adding audio track')\n",
    "%rm vocals.mp4\n",
    "!ffmpeg -i temp.mp4 -i \"vocals.wav\" -c:v copy -map 0:v:0 -map 1:a:0 -loglevel error vocals.mp4\n",
    "# REPLACE for accompaniment with:\n",
    "# %rm accompaniment.mp4\n",
    "# !ffmpeg -i temp.mp4 -i \"accompaniment.wav\" -c:v copy -map 0:v:0 -map 1:a:0 -loglevel error accompaniment.mp4\n",
    "                          \n",
    "\n",
    "# remove temporary files\n",
    "%rm temp.mp4\n",
    "%rm -r /content/img/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the vocals and accompaniment videos\n",
    "# to generate the video for the accompaniment, rerun all the cells after the section \"Feature extraction\" \n",
    "# while making the replacements in the comments \"REPLACE\"\n",
    "!ffmpeg -i vocals.mp4 -i accompaniment.mp4 -filter_complex hstack temp.mp4 -y\n",
    "!ffmpeg -i temp.mp4 -i \"audio.wav\" -c:v copy -map 0:v:0 -map 1:a:0 -loglevel error output.mp4 -y\n",
    "%rm temp.mp4\n",
    "print('saved output video with audio to output.mp4')\n",
    "# download and play test_audio.mp4 to see the results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Music Visualizer",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
